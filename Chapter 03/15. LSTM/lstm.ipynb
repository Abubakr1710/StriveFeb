{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21f9528d130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ISE</th>\n",
       "      <th>ISE.1</th>\n",
       "      <th>SP</th>\n",
       "      <th>DAX</th>\n",
       "      <th>FTSE</th>\n",
       "      <th>NIKKEI</th>\n",
       "      <th>BOVESPA</th>\n",
       "      <th>EU</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-Jan-09</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.028524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6-Jan-09</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.007787</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.008773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-Jan-09</td>\n",
       "      <td>-0.028862</td>\n",
       "      <td>-0.026353</td>\n",
       "      <td>-0.030469</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>-0.028735</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>-0.017073</td>\n",
       "      <td>-0.020015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8-Jan-09</td>\n",
       "      <td>-0.062208</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>-0.005561</td>\n",
       "      <td>-0.019424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9-Jan-09</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>-0.021533</td>\n",
       "      <td>-0.019873</td>\n",
       "      <td>-0.012710</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>-0.007802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date       ISE     ISE.1        SP       DAX      FTSE    NIKKEI  \\\n",
       "0  5-Jan-09  0.035754  0.038376 -0.004679  0.002193  0.003894  0.000000   \n",
       "1  6-Jan-09  0.025426  0.031813  0.007787  0.008455  0.012866  0.004162   \n",
       "2  7-Jan-09 -0.028862 -0.026353 -0.030469 -0.017833 -0.028735  0.017293   \n",
       "3  8-Jan-09 -0.062208 -0.084716  0.003391 -0.011726 -0.000466 -0.040061   \n",
       "4  9-Jan-09  0.009860  0.009658 -0.021533 -0.019873 -0.012710 -0.004474   \n",
       "\n",
       "    BOVESPA        EU        EM  \n",
       "0  0.031190  0.012698  0.028524  \n",
       "1  0.018920  0.011341  0.008773  \n",
       "2 -0.035899 -0.017073 -0.020015  \n",
       "3  0.028283 -0.005561 -0.019424  \n",
       "4 -0.009764 -0.010989 -0.007802  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('data_akbilgic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(df.shape[0]*0.8)\n",
    "df_train = df[:train_size]\n",
    "df_test = df[train_size:]\n",
    "\n",
    "df_train = df_train.values\n",
    "df_test =df_test.values\n",
    "\n",
    "feat= df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 9)\n"
     ]
    }
   ],
   "source": [
    "# Receives the number of samples (batch_size) of size (n_steps) to extract\n",
    "# from the time series, and outputs such a sample\n",
    "def next_stock_batch(batch_size, n_steps, feat, n_features):\n",
    "    t_min = 0\n",
    "    t_max = feat.shape[0]\n",
    "  \n",
    "    # The inputs will be formed by 8 sequences taken from\n",
    "    # 7 time series [ISE.1,SP,DAX,FTSE,NIKKEI,BOVESPA,EU]\n",
    "    x = np.zeros((batch_size,n_steps,n_features))\n",
    "    \n",
    "    # We want to predict the returns of the Istambul stock\n",
    "    # taken into consideration the previous n_steps days\n",
    "    y = np.zeros((batch_size,n_steps))\n",
    "\n",
    "    # We chose batch_size random points from time series x-axis\n",
    "\n",
    "    starting_points = np.random.randint(0,t_max-n_steps-1,size=batch_size)    \n",
    "    #print(starting_points)\n",
    "    #print(feat.shape)\n",
    "    \n",
    "    # We create the batches for x using all time series (8) between t and t+n_steps]\n",
    "    for i, sp in enumerate(starting_points):\n",
    "        x[i] = feat[sp: sp+n_steps]\n",
    "        y[i] = feat[sp+1:sp+n_steps+1, 1]    \n",
    "    # We create the batches for y using only one time series between t+1 and t+n_steps+1\n",
    "    \n",
    "    #Save on x and y the time series data sequence and the prediction sequence\n",
    "\n",
    "    return x,y\n",
    "\n",
    "x,y =  next_stock_batch(batch_size=32, n_steps=10, feat=feat, n_features=9)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size, seq_len):\n",
    "        super(). __init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size =batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first =True)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.batch_size*self.hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, self.batch_size*self.seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0=torch.zeros((self.num_layers, self.batch_size, self.hidden_size ))\n",
    "        c_0=torch.zeros((self.num_layers, self.batch_size, self.hidden_size ))\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x,(h_0,c_0))\n",
    "        last_hidden = h_n[-1]\n",
    "        #print(last_hidden.shape)\n",
    "        #ans torch.Size([10, 32, 5]), n_steps=10, batch_size=32, hidden_size=5\n",
    "\n",
    "        x = F.relu(last_hidden.flatten()) # added this line, you can activate also the last hidden layer, for better performance\n",
    "        #print(x.shape)\n",
    "        #ans: torch.Size([1600]) if you multiply that ([10, 32, 5]) you will get\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # error in here: x = F.relu(self.fc1(x)),  self.fc1=nn.Linear(self.batch_size*self.hidden_size, 1024)\n",
    "        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1600 and 160x1024)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "# h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size)\n",
    "# c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size)\n",
    "# x, _ = self.BELT_LSTM(x, (h_0, c_0))\n",
    "\n",
    "model = LSTM(input_size=9, hidden_size=5, num_layers=10, batch_size=32, seq_len=10)\n",
    "nx= torch.from_numpy(x).float()\n",
    "#print(nx.shape)\n",
    "#ans torch.Size([32, 10, 9]), batch_size=32, n_steps=10, feat=feat, n_features=9\n",
    "pred=model(nx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize our RNN model to pass it to the optimizer\n",
    "\n",
    "model = LSTM(df.shape[1],5, 10, 32, 10)\n",
    "\n",
    "# What would be an adecuate loss function?\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer to apply the gradients\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03504864498972893\n",
      "MSE: 0.019334690645337105\n",
      "MSE: 0.006604543421417475\n",
      "MSE: 0.0022303410805761814\n",
      "MSE: 0.0011081616394221783\n",
      "MSE: 0.0006641888758167624\n",
      "MSE: 0.0005223475745879114\n",
      "MSE: 0.00048250198597088456\n",
      "MSE: 0.0004956505144946277\n"
     ]
    }
   ],
   "source": [
    "#number of batches we will go through\n",
    "batch_size= 32\n",
    "#how many squence there will be in a bacth\n",
    "n_iterations = 500\n",
    "#after how many operations we will print information\n",
    "printing_gap = 50\n",
    "\n",
    "#We will store the loss values here to plot them\n",
    "train_loss = []\n",
    "\n",
    "for iter in range(n_iterations):\n",
    "    #Get a batch\n",
    "\n",
    "    X_train, y_train = next_stock_batch(batch_size=32, n_steps=10, feat=df_train, n_features=9)\n",
    "\n",
    "    #make into tensor\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    #make them into torch variables in float format\n",
    "\n",
    "    #Reset the gradients\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #Get the outputs\n",
    "    pred=model(X_train)\n",
    "    #detach the hidden state\n",
    "    \n",
    "    #compute the loss\n",
    "    loss = criterion(pred, y_train.flatten())\n",
    "    #compute the gradients\n",
    "    loss.backward()\n",
    "    #Apply the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    #Append the loss value\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    if iter % printing_gap == 0:\n",
    "        print(f'MSE: {loss.item()}')\n",
    "        #Print the information\n",
    "\n",
    "\n",
    "plt.plot(train_loss, label= \"Train Loss\")\n",
    "plt.xlabel(\" Iteration \")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c783155fdf7cc6e25183d446515f6b6ba379df7b28dd698d21634d1b4d5e58fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
