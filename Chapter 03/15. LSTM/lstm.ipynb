{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ISE</th>\n",
       "      <th>ISE.1</th>\n",
       "      <th>SP</th>\n",
       "      <th>DAX</th>\n",
       "      <th>FTSE</th>\n",
       "      <th>NIKKEI</th>\n",
       "      <th>BOVESPA</th>\n",
       "      <th>EU</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-Jan-09</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.028524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6-Jan-09</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.007787</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.008773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-Jan-09</td>\n",
       "      <td>-0.028862</td>\n",
       "      <td>-0.026353</td>\n",
       "      <td>-0.030469</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>-0.028735</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>-0.017073</td>\n",
       "      <td>-0.020015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8-Jan-09</td>\n",
       "      <td>-0.062208</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>-0.005561</td>\n",
       "      <td>-0.019424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9-Jan-09</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>-0.021533</td>\n",
       "      <td>-0.019873</td>\n",
       "      <td>-0.012710</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>-0.007802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date       ISE     ISE.1        SP       DAX      FTSE    NIKKEI  \\\n",
       "0  5-Jan-09  0.035754  0.038376 -0.004679  0.002193  0.003894  0.000000   \n",
       "1  6-Jan-09  0.025426  0.031813  0.007787  0.008455  0.012866  0.004162   \n",
       "2  7-Jan-09 -0.028862 -0.026353 -0.030469 -0.017833 -0.028735  0.017293   \n",
       "3  8-Jan-09 -0.062208 -0.084716  0.003391 -0.011726 -0.000466 -0.040061   \n",
       "4  9-Jan-09  0.009860  0.009658 -0.021533 -0.019873 -0.012710 -0.004474   \n",
       "\n",
       "    BOVESPA        EU        EM  \n",
       "0  0.031190  0.012698  0.028524  \n",
       "1  0.018920  0.011341  0.008773  \n",
       "2 -0.035899 -0.017073 -0.020015  \n",
       "3  0.028283 -0.005561 -0.019424  \n",
       "4 -0.009764 -0.010989 -0.007802  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('data_akbilgic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(df.shape[0]*0.8)\n",
    "df_train = df[:train_size]\n",
    "df_test = df[train_size:]\n",
    "\n",
    "df_train = df_train.values\n",
    "df_test =df_test.values\n",
    "\n",
    "feat= df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 9)\n"
     ]
    }
   ],
   "source": [
    "# Receives the number of samples (batch_size) of size (n_steps) to extract\n",
    "# from the time series, and outputs such a sample\n",
    "def next_stock_batch(batch_size, n_steps, feat, n_features):\n",
    "    t_min = 0\n",
    "    t_max = feat.shape[0]\n",
    "  \n",
    "    # The inputs will be formed by 8 sequences taken from\n",
    "    # 7 time series [ISE.1,SP,DAX,FTSE,NIKKEI,BOVESPA,EU]\n",
    "    x = np.zeros((batch_size,n_steps,n_features))\n",
    "    \n",
    "    # We want to predict the returns of the Istambul stock\n",
    "    # taken into consideration the previous n_steps days\n",
    "    y = np.zeros((batch_size,n_steps))\n",
    "\n",
    "    # We chose batch_size random points from time series x-axis\n",
    "\n",
    "    starting_points = np.random.randint(0,t_max-n_steps-1,size=batch_size)    \n",
    "    #print(starting_points)\n",
    "    #print(feat.shape)\n",
    "    \n",
    "    # We create the batches for x using all time series (8) between t and t+n_steps]\n",
    "    for i, sp in enumerate(starting_points):\n",
    "        x[i] = feat[sp: sp+n_steps]\n",
    "        y[i] = feat[sp+1:sp+n_steps+1, 1]    \n",
    "    # We create the batches for y using only one time series between t+1 and t+n_steps+1\n",
    "    \n",
    "    #Save on x and y the time series data sequence and the prediction sequence\n",
    "\n",
    "    return x,y\n",
    "\n",
    "x,y =  next_stock_batch(batch_size=32, n_steps=10, feat=feat, n_features=9)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 9])\n",
      "torch.Size([10, 32, 5])\n",
      "flatten: tensor([ 0.1073,  0.1069, -0.2380,  ...,  0.1670, -0.1073,  0.3813],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "torch.Size([1600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1600 and 160x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abubakr\\Documents\\GitHub\\StriveFeb\\Chapter 03\\15. LSTM\\lstm.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(nx\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=39'>40</a>\u001b[0m \u001b[39m#ans torch.Size([32, 10, 9]), batch_size=32, n_steps=10, feat=feat, n_features=9\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=40'>41</a>\u001b[0m pred\u001b[39m=\u001b[39mmodel(nx)\n",
      "File \u001b[1;32mc:\\Users\\Abubakr\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Abubakr\\Documents\\GitHub\\StriveFeb\\Chapter 03\\15. LSTM\\lstm.ipynb Cell 6'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=24'>25</a>\u001b[0m \u001b[39m#ans: torch.Size([1600]) if you multiply that ([10, 32, 5]) you will get\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=27'>28</a>\u001b[0m \u001b[39m# error in here: x = F.relu(self.fc1(x)),  self.fc1=nn.Linear(self.batch_size*self.hidden_size, 1024)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=28'>29</a>\u001b[0m \u001b[39m# RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1600 and 160x1024)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abubakr/Documents/GitHub/StriveFeb/Chapter%2003/15.%20LSTM/lstm.ipynb#ch0000005?line=29'>30</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\Abubakr\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Abubakr\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1600 and 160x10)"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size, seq_len):\n",
    "        super(). __init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size =batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first =True)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.batch_size*self.hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, self.batch_size*self.seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0=torch.zeros((self.num_layers, self.batch_size, self.hidden_size ))\n",
    "        c_0=torch.zeros((self.num_layers, self.batch_size, self.hidden_size ))\n",
    "\n",
    "        lstm_out, h_n = self.lstm(x,(h_0,c_0))\n",
    "        last_hidden = h_n[-1]\n",
    "        print(last_hidden.shape)\n",
    "        #ans torch.Size([10, 32, 5]), n_steps=10, batch_size=32, hidden_size=5\n",
    "\n",
    "        x = F.relu(last_hidden.flatten()) # added this line, you can activate also the last hidden layer, for better performance\n",
    "        print(x.shape)\n",
    "        #ans: torch.Size([1600]) if you multiply that ([10, 32, 5]) you will get\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # error in here: x = F.relu(self.fc1(x)),  self.fc1=nn.Linear(self.batch_size*self.hidden_size, 1024)\n",
    "        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1600 and 160x1024)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "# h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size)\n",
    "# c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size)\n",
    "# x, _ = self.BELT_LSTM(x, (h_0, c_0))\n",
    "\n",
    "model = LSTM(input_size=9, hidden_size=5, num_layers=10, batch_size=32, seq_len=10)\n",
    "nx= torch.from_numpy(x).float()\n",
    "print(nx.shape)\n",
    "#ans torch.Size([32, 10, 9]), batch_size=32, n_steps=10, feat=feat, n_features=9\n",
    "pred=model(nx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c783155fdf7cc6e25183d446515f6b6ba379df7b28dd698d21634d1b4d5e58fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
