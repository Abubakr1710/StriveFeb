{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-fiction",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-burning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"import spacy\\n\\n%load_ext nb_black\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n                var nbb_formatted_code = \"import spacy\\n\\n%load_ext nb_black\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "%load_ext nb_black\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-water",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(u\\\"Hello, world. Antonio is learning Python.\\\")\";\n                var nbb_formatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(\\\"Hello, world. Antonio is learning Python.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-covering",
   "metadata": {},
   "source": [
    "## Get tokens and sentences\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
    "\n",
    "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
    "\n",
    "\t\"Antonio is learning Python!\"\n",
    "\t[\"Antonio\",\"is\",\"learning\",\"Python!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "painted-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n                var nbb_formatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-america",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Jan\", \"Javier\", \"Antonio\", \"Italy\"\n",
    "3. INTJ, Interjection - \"Ohhhhhhhhhhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello INTJ\n",
      ", PUNCT\n",
      "world NOUN\n",
      ". PUNCT\n",
      "Antonio PROPN\n",
      "is AUX\n",
      "learning VERB\n",
      "Python PROPN\n",
      ". PUNCT\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"# For each token, print corresponding part of speech tag\\nfor token in doc:\\n    print(token.text, token.pos_)\";\n                var nbb_formatted_code = \"# For each token, print corresponding part of speech tag\\nfor token in doc:\\n    print(token.text, token.pos_)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "english-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"from spacy import displacy\";\n                var nbb_formatted_code = \"from spacy import displacy\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inner-surgeon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f4180138ed164d6387526d0869272fa2-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hello,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">world.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Antonio</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Python.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4180138ed164d6387526d0869272fa2-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4180138ed164d6387526d0869272fa2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M220.0,179.0 L228.0,167.0 212.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4180138ed164d6387526d0869272fa2-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4180138ed164d6387526d0869272fa2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4180138ed164d6387526d0869272fa2-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4180138ed164d6387526d0869272fa2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4180138ed164d6387526d0869272fa2-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4180138ed164d6387526d0869272fa2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"displacy.render(doc, style='dep')\";\n                var nbb_formatted_code = \"displacy.render(doc, style=\\\"dep\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-rabbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello, world. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Antonio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is learning \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Python\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"displacy.render(doc, style = \\\"ent\\\",jupyter = True)\";\n                var nbb_formatted_code = \"displacy.render(doc, style=\\\"ent\\\", jupyter=True)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = \"ent\",jupyter = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28cc3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"print(doc[2])\";\n                var nbb_formatted_code = \"print(doc[2])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-typing",
   "metadata": {},
   "source": [
    "We have said that dependency structures are represented by directed graphs that satisfy the following constraints:\n",
    "\n",
    "1. There is a single designated root node that has no incoming arcs.\n",
    "\n",
    "2. With the exception of the root node, each vertex has exactly one incoming arc.\n",
    "\n",
    "3. There is a unique path from the root node to each vertex in V.\n",
    "\n",
    "You can inspect the head of each token by invoking the `.head` attribute of a spaCy token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "premium-violence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"doc[2]\";\n                var nbb_formatted_code = \"doc[2]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "remarkable-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"doc[2].head\";\n                var nbb_formatted_code = \"doc[2].head\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc[2].head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-character",
   "metadata": {},
   "source": [
    "So how would you search for the root?\n",
    "\n",
    "Since there is a unique path from the root node to each vertex in V, there's only one root node that has no incoming arcs, we can search for the token which have as head itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "celtic-armstrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "learning\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"for token in doc:\\n    if token.head == token:\\n        print(token)\";\n                var nbb_formatted_code = \"for token in doc:\\n    if token.head == token:\\n        print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.head == token:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-ground",
   "metadata": {},
   "source": [
    "As expected, since there were two sentences in the doc, we got two roots.\n",
    "\n",
    "We can also build a function that, given a spaCy token, gives the path till the root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threaded-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "learning\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"# Define a function to find the path to the root of each word in a sentence\\n\\ndef path_to_the_root(pth):\\n    for token in pth:\\n        if token.head == token:\\n            print(token)\\n\\npath_to_the_root(doc)\";\n                var nbb_formatted_code = \"# Define a function to find the path to the root of each word in a sentence\\n\\n\\ndef path_to_the_root(pth):\\n    for token in pth:\\n        if token.head == token:\\n            print(token)\\n\\n\\npath_to_the_root(doc)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to find the path to the root of each word in a sentence\n",
    "\n",
    "def path_to_the_root(pth):\n",
    "    for token in pth:\n",
    "        if token.head == token:\n",
    "            print(token)\n",
    "\n",
    "path_to_the_root(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "united-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "learning\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"path_to_the_root(doc)\";\n                var nbb_formatted_code = \"path_to_the_root(doc)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_the_root(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73c0d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"print(doc[4])\";\n                var nbb_formatted_code = \"print(doc[4])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(doc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "An embedding is a fixed sizes numerical vector that attempts to encode some semantic meaning of the word or sentence it is encoding. The distributional hypothesis is usually the concept behind most embeddings. This hypothesis states that words which often have the same neighboring words tend to be semantically similar. For example if 'football' and 'basketball' usually appear close the word 'play' we assume that they will be semantically similar. An algorithm that is based on this concept is Word2Vec. A common way of obtaining sentence embeddings is to average the word embeddings inside the sentence and use that average as the representation of the whole sentence. \n",
    "\n",
    "- In spacy every token has its embedding.\n",
    "- It is under the attribute 'vector'.\n",
    "- In spacy embeddings are of size 96 or 128.\n",
    "\n",
    "\n",
    "Obtain the embeddings of all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0823821  -0.42946622 -0.5029499   0.16072363  0.6249163  -0.43111977\n",
      "  1.4949617   0.84327805  1.1696405  -1.146951    0.22747783  1.609772\n",
      " -0.5234932  -0.6649049  -0.43582094  0.55953836 -0.15735066 -1.0758792\n",
      "  0.6815688  -0.05594122 -0.14967234 -0.36605948 -0.3892722   0.46946785\n",
      " -0.35326886 -0.15151012 -0.07002574  0.01581563 -0.7716576   0.16755503\n",
      "  0.50840676 -1.330525   -0.84367275  0.76433086 -0.49710384  0.20579234\n",
      " -1.0266633  -0.42367968  0.1842682   0.9837595   0.35812497 -0.04406814\n",
      " -0.13158312 -1.0771542  -0.07300432  0.02702707  0.10031849 -0.6668478\n",
      "  0.69046175 -0.6554684  -1.0477257  -0.25338298 -0.84254164 -0.3190821\n",
      "  0.13100918 -0.1516825  -0.8030119  -1.0564109  -0.5003395   1.9936432\n",
      "  2.533157   -0.04505131  1.0323157   0.18503037  0.78973675  0.9035241\n",
      " -0.60990644  0.02243002 -0.9370111  -1.0024397   0.54751515 -0.83813816\n",
      " -0.82515997 -0.9111524   1.2109123  -1.2123573  -0.29819542 -0.24432868\n",
      "  0.33222318  0.30747586  0.08078419 -1.6028439   0.11524671  0.4370293\n",
      "  2.5643036  -0.8824997  -0.7356517  -0.20263758  0.31787246 -0.731652\n",
      " -0.1390136  -0.30552733 -0.21013585  0.37730932  1.0254644   0.2692515 ]\n",
      "[ 0.8578079  -0.5549769   0.34712318 -0.24227363 -1.0295969  -0.9015329\n",
      " -0.00955319 -0.12061432 -2.0041873   1.0457537   0.5846596   0.10328315\n",
      " -0.98217916 -0.544333   -0.07133992 -0.98894334  0.5907095   1.4221653\n",
      " -0.03749165 -0.9717886  -0.18998623  0.05305497 -1.047044   -0.12895066\n",
      " -1.6614152   1.0520235   1.1360028  -1.0342981  -0.3678879   0.2613001\n",
      " -0.73585075  1.559641   -1.0942073   0.03867123  0.80039895 -1.1658969\n",
      " -0.06541345  0.6359631   1.2351367  -0.40751255 -0.22440262 -0.2389788\n",
      " -0.50461674  0.01882051 -0.22964355  0.04443586  0.7378051   1.1638248\n",
      "  0.7903136  -0.29033458  0.77740645 -1.2195429   0.19618921 -0.5613128\n",
      " -0.12307662 -0.2584412  -0.83960795  0.22331583  0.3260579   2.3239164\n",
      " -0.14099525 -0.44637874 -0.64822197  0.5959455   0.962898    0.60793036\n",
      "  0.41913956 -0.36615103  1.4617962   0.8040593   0.23228669 -1.0517883\n",
      "  0.08720095 -0.7460309  -0.05831836 -0.28607392 -0.70701903  0.08815132\n",
      "  1.2083237   1.4309746  -0.38863334 -0.33496934 -0.54301345 -0.4342047\n",
      " -0.9759546   0.9713502  -0.2613962   0.5800071  -1.39973    -0.06351365\n",
      "  0.60174906  0.6969775  -0.0723689  -0.43664795  0.09621847  2.009554  ]\n",
      "[ 1.3139932  -0.40480572  0.57618225 -0.51682335 -0.895352   -0.4087513\n",
      "  0.5276739  -0.95927334  1.0144682  -0.13405451 -0.83890283 -0.612737\n",
      "  1.2515522  -0.4464346   1.5830554  -0.81772786 -0.05669132 -0.15073235\n",
      " -0.16164762  0.06724679 -0.19498493 -1.8191756   0.54897726  0.68341553\n",
      " -0.3331847   1.8164408  -0.66382015  0.37070107  0.43547857 -0.49044764\n",
      " -0.2728176   0.13598351 -0.97940814 -0.03283678 -0.07788709 -0.34848598\n",
      " -0.14117646  0.5220264  -0.21706292 -0.2846061   0.91514945 -1.103236\n",
      " -0.47262686 -0.32969767 -0.9290342   0.82048774  0.51593167  0.1889569\n",
      " -0.28601307  0.8281294  -0.02783711 -0.4732564  -0.44739568  0.04226363\n",
      " -0.3713242  -0.32500085  0.2722922   0.19574182 -0.24687439  0.07929701\n",
      " -0.18736957 -0.9557237  -0.10852781  0.19495384 -0.40630087 -0.05616665\n",
      "  0.5090701   0.38253158 -0.34937263 -1.3589573   0.96250117 -1.1175084\n",
      " -0.753984   -0.13866395 -0.3481394   0.5085404   0.46541846 -0.34282872\n",
      "  0.06463292  1.2432426   1.1747178  -0.18333177  0.17028952  0.94291025\n",
      "  0.40106633  0.9215603   0.59242773  0.12153403  1.2487227  -0.1820795\n",
      " -1.1135204   0.7916778  -1.314195    0.16218773  0.32430494  0.70919573]\n",
      "[-1.31703126e+00  2.66718298e-01  9.75017488e-01 -5.77854574e-01\n",
      " -8.42639208e-02 -1.11036229e+00 -2.71851122e-02 -8.29746306e-01\n",
      " -4.54251707e-01 -2.93693364e-01  4.02582020e-01 -1.41270816e+00\n",
      " -6.99296772e-01 -4.01071310e-01 -1.15033698e+00  3.04461181e-01\n",
      " -8.37535024e-01 -2.12756082e-01  3.62994003e+00 -2.56622016e-01\n",
      "  3.41050506e-01 -9.78857428e-02 -1.14194465e+00 -1.51774019e-01\n",
      "  2.90177226e-01  3.50444937e+00 -4.09499884e-01  7.38722205e-01\n",
      "  1.28655374e+00 -1.29166961e+00 -1.33239925e-01 -1.71863735e-02\n",
      " -4.71659720e-01  8.25540364e-01 -1.74494743e-01 -6.21545911e-02\n",
      "  4.37314898e-01  6.45273805e-01 -3.79842937e-01 -1.44774520e+00\n",
      "  9.19731408e-02 -9.44333553e-01 -4.76299107e-01  3.55927289e-01\n",
      "  4.79334563e-01 -4.73594338e-01 -7.19595432e-01 -9.59437907e-01\n",
      " -4.10286337e-03  2.71218252e+00 -1.38217688e-01 -4.45362568e-01\n",
      "  2.59843349e-01  6.59957170e-01  3.42352271e-01 -4.86717463e-01\n",
      " -1.24964750e+00  7.68359065e-01 -8.19630325e-02 -1.07732010e+00\n",
      " -1.43636417e+00  1.01193678e+00 -6.64845109e-01 -9.51149344e-01\n",
      " -7.16334581e-03  1.47667873e+00 -5.29840589e-02  7.48613596e-01\n",
      "  5.63162446e-01  8.31841588e-01  2.39088684e-02 -5.62604904e-01\n",
      " -1.70813489e+00  3.10324430e-02  2.20391798e+00  5.73782861e-01\n",
      "  6.68367445e-02 -6.88697755e-01 -3.07501078e-01 -4.48733687e-01\n",
      " -1.33026731e+00  1.19947195e-02 -7.30112344e-02  8.17725360e-01\n",
      " -6.99068367e-01 -2.73681283e-01  2.59740233e-01 -5.36653161e-01\n",
      " -1.49106458e-01 -8.27749610e-01 -2.70272195e-01  4.77906275e+00\n",
      " -1.05165648e+00 -3.49708617e-01  2.06041470e-01  8.84807467e-01]\n",
      "[ 0.29934216 -0.34660992 -0.4406328  -0.39567018 -0.00664223 -0.8426498\n",
      "  0.4742052  -0.23316449 -0.07877816 -0.88053083  0.29865095  1.117529\n",
      " -0.28715152 -0.05186561 -0.6469054   0.31411743 -0.28503612 -1.0481355\n",
      "  0.5709848  -1.0161217   1.105649   -0.62134224  0.09180934 -0.02241993\n",
      "  1.7768139   0.8268226   0.18103763 -0.08307147 -0.9455478   0.05663063\n",
      "  0.81098354 -0.7000804  -1.6891791  -0.04858682 -1.8866009  -0.4569046\n",
      " -0.74825114  1.4180856   0.08985643 -0.16517891 -0.05382088 -0.7878069\n",
      " -0.39774168  0.10140055 -2.138963    0.06680021  0.27719477 -1.5908421\n",
      "  1.5114787  -0.4318567   0.48537263 -0.4594579  -0.17523201 -0.71383333\n",
      "  1.1767267   0.3626518  -0.04032962 -0.13258775  0.5540115  -0.8312658\n",
      "  3.4117732   0.8104322   0.6614342  -0.02909136 -1.351902    0.23675364\n",
      " -0.04132696 -0.61950296 -0.2450995  -0.28833726 -0.09707914  1.0242829\n",
      " -1.0444496   0.56372    -0.01033391  1.153727   -0.6454965  -0.17459416\n",
      " -0.3409667   0.8987417  -1.1918247  -1.0689514  -0.1933346   1.8083913\n",
      "  0.43273512 -1.1721678  -0.08552566  0.9177225   0.8319652  -0.69470215\n",
      "  0.01781086 -0.4294573   1.0630703   0.05854734 -0.7733395   0.61565864]\n",
      "[ 0.7924104  -0.8094774   0.53671557 -0.45625412 -0.2077173   1.9473224\n",
      "  0.9917762   0.77721465  0.07151607  0.7493466   1.8469148  -0.30925435\n",
      " -0.61630774 -0.69703007  0.824088   -1.6606064  -0.42332557  0.30643266\n",
      " -1.2146055  -0.18872239  0.9107946   0.9543849  -1.1654931  -1.7641468\n",
      " -0.9844734   0.28432342  0.38370612 -0.5575438  -1.3389459   0.3803651\n",
      "  0.56450546  1.1401886   1.0211775   1.4818943  -0.30834913 -0.4526294\n",
      "  0.527121    0.40624696  1.0975982  -0.35912955 -0.16836709 -0.43965343\n",
      "  0.6906158  -0.57572484 -0.7907325  -0.6102888  -0.71988845 -1.0837619\n",
      " -0.5493109  -0.742151    0.25579613  0.57657933  0.87219244  0.96529007\n",
      "  0.22857834 -0.6187033  -1.1327237   0.9976045  -1.2529206  -0.91850007\n",
      "  0.24411732 -1.0278971  -0.07400858 -0.03498524 -0.30380893 -0.57471347\n",
      " -0.072191    0.69031584 -1.0142207   0.22069156  0.15367743  0.517539\n",
      " -0.62107456  1.8357726   0.20078588  0.55750716 -0.5630432  -0.53058845\n",
      " -0.31808347 -0.38484374 -0.05033876  0.7328499   0.46859953  0.23504543\n",
      " -0.32149893  1.086129   -0.61998177 -0.17484893 -1.2446408  -0.2717979\n",
      " -1.0062788   0.19246867  0.8950231   1.4212466   0.9386549  -0.82715285]\n",
      "[-1.3886553  -0.41430375 -0.6531945   0.8111882  -0.28008884 -0.03540546\n",
      " -0.6762013   0.4059      0.54031944 -0.36633152  0.02712965 -0.01431181\n",
      "  0.00323655 -0.19037908 -0.65812004 -0.32780197  0.20931533  0.06221443\n",
      " -1.6503764   0.8318777   2.8924146  -0.8704177   0.05121788 -0.7411618\n",
      " -0.5020416   1.2572818   0.25712648  1.0795822   1.018778   -0.6817121\n",
      "  1.3542577   1.7320229   1.7966197  -0.9806409   0.08199823  0.33609265\n",
      "  0.9455627  -1.3749596   1.2869717  -0.35819614  0.08563484  0.66477746\n",
      "  1.6066592  -0.6583325   0.4778091  -0.43569776 -0.8999435  -0.2691231\n",
      " -0.28326166  0.8265308   0.2390441  -0.0048678  -0.7995786  -0.15544492\n",
      "  0.8822011   0.9733879  -2.0198667   0.2112929  -0.5469164  -0.5939485\n",
      "  0.07815215 -0.9101491   0.08957408  0.41165304 -1.2491759   0.43289685\n",
      "  0.0856152  -0.23106226 -0.3002025  -0.25067183  0.9770638   0.41853863\n",
      " -0.5553558  -0.24233699 -0.11354874  0.391835    0.85192716 -1.051651\n",
      "  0.24467388  0.32590246  0.58487046 -1.3986675   0.88109773 -0.78629696\n",
      " -1.0070417  -1.4899317  -0.02130149  0.5873487   1.9472389  -0.33946157\n",
      " -1.2302549  -0.05915383 -0.2113409  -0.14625096  0.29960325 -0.36635092]\n",
      "[ 0.069828   -0.24076042 -0.16577137  2.1431293   0.33021218  0.7510983\n",
      "  0.90498334  0.33123723  1.0742625  -0.65420234  0.41558045 -0.12055722\n",
      "  0.31416357  0.5716463   0.88607955 -0.07776579 -0.61521864 -0.08897609\n",
      " -1.0162255  -0.18633434  0.07584365 -0.13681868  0.08045135  0.41530293\n",
      "  0.13663471  1.1188705   0.7842056  -0.03125692 -0.21468136 -0.42886093\n",
      "  0.3720826  -0.6663778  -0.7727085  -2.195223   -0.84713674 -0.40400153\n",
      "  0.07305428 -0.33240455  0.2064681  -0.8088039  -1.0627465   0.09903806\n",
      " -0.03762778 -0.629274    1.9081658  -0.9097624   0.02000162 -1.0869721\n",
      "  0.19063401 -0.31121212 -0.16671564  0.07728875 -0.60344374 -0.91603935\n",
      "  0.34559172 -0.8045404   1.6202002  -1.0455009  -0.21786083  0.7361876\n",
      " -0.6066675  -0.37564164  0.11330301  0.5139327  -0.9623945   0.15105522\n",
      "  0.84162366  0.75506616 -0.99695015 -0.39500773  0.523275   -0.96055865\n",
      " -0.5109547   0.28133643 -0.08378294  0.5279244   0.26219594 -1.1613047\n",
      "  0.28972733  1.1697267   0.735185   -0.9065056   0.07450111 -0.34981072\n",
      "  1.0586056  -0.48005462  0.15644541  0.5316416   0.11062934  0.9398109\n",
      " -0.25972933  1.4680704  -0.4731925   0.06063825 -0.7331257   0.64739996]\n",
      "[-7.7609479e-01  1.1175338e+00  1.4569305e+00 -3.5946327e-01\n",
      " -7.0587510e-01  5.5780113e-01 -1.3683641e-01 -5.9381348e-01\n",
      "  5.2787793e-01  6.5990472e-01  5.1613766e-01 -6.7836368e-01\n",
      " -2.3524003e-01 -7.4590778e-01 -1.6737368e+00  3.3949172e-01\n",
      " -1.6608160e+00  2.1085653e+00  4.3282499e+00 -5.7124835e-01\n",
      "  1.5494615e-01 -7.2307968e-01 -1.5896823e+00 -9.3928498e-01\n",
      " -2.4841189e-01  1.5924778e+00 -1.5930909e-01  7.6526171e-01\n",
      "  3.1041238e-01 -9.8358268e-01  9.7994399e-01 -7.9965159e-02\n",
      " -2.6308000e-04  5.4765701e-01  2.0453196e+00  1.3010042e+00\n",
      "  3.3042401e-01  6.3609070e-01  4.7822797e-01 -1.4210503e+00\n",
      " -5.1003003e-01 -1.7644821e-01 -4.9561244e-01 -7.5599682e-01\n",
      "  7.1350181e-01 -1.4931296e+00 -2.5991988e-01 -2.1662929e+00\n",
      "  7.7018988e-01  2.5447673e-01  3.1763408e-01 -2.9476231e-01\n",
      "  9.2875975e-01  1.1655688e+00  8.6030328e-01 -1.7778682e+00\n",
      " -1.8445077e+00 -4.6540648e-02 -5.0610352e-01 -2.1289819e-01\n",
      " -4.4308448e-01 -7.9275727e-02 -5.8888978e-01 -4.2778236e-01\n",
      "  9.8833382e-01  3.8996819e-01  5.9632307e-01  1.0694588e+00\n",
      "  1.1436901e+00  7.9905921e-01  7.1645522e-01 -3.7872189e-01\n",
      " -1.3376504e+00 -9.7053796e-01 -1.9767688e-01 -1.0897565e+00\n",
      " -2.2547689e-01 -8.6854994e-03  6.8436468e-01 -9.3143559e-01\n",
      " -6.7236459e-01 -3.8106906e-01  1.6739701e-01  1.4690942e+00\n",
      " -4.7988272e-01 -6.8412393e-01 -7.6569155e-02 -3.3077195e-01\n",
      " -5.3829396e-01 -4.5046523e-01  2.0261598e-01  4.8894229e+00\n",
      " -9.1646123e-01 -2.8658903e-01 -1.7448735e-01  9.7896129e-01]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"def path_to_the_root(pth):\\n    for token in pth:\\n        print(token.vector)\\n\\n\\npath_to_the_root(doc)\";\n                var nbb_formatted_code = \"def path_to_the_root(pth):\\n    for token in pth:\\n        print(token.vector)\\n\\n\\npath_to_the_root(doc)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def path_to_the_root(pth):\n",
    "    for token in pth:\n",
    "        print(token.vector)\n",
    "\n",
    "\n",
    "path_to_the_root(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity \n",
    "\n",
    "To compute the semantic similarity between two sentences, $u$ and $v$, we measure the cossine similarity between the two sentence embeddings. The formula is as follows:\n",
    "\n",
    "$sim(u, v) = \\frac{u \\cdot v}{||u|| ||v||} $\n",
    "\n",
    "\n",
    "Use the following formula to get the semantic similarity betwen the words in doc.\n",
    "Feel free to test it between differente words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 17;\n                var nbb_unformatted_code = \"import numpy as np\\ndef semantic_sim(u,v):\\n    return np.dot(u,v)/(abs(v)*abs(u))\";\n                var nbb_formatted_code = \"import numpy as np\\n\\n\\ndef semantic_sim(u, v):\\n    return np.dot(u, v) / (abs(v) * abs(u))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def semantic_sim(u,v):\n",
    "    return np.dot(u,v)/(abs(v)*abs(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-virgin",
   "metadata": {},
   "source": [
    "# Pride and Prejudice analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-danger",
   "metadata": {},
   "source": [
    "We would like to:\n",
    "\n",
    "- Extract the names of all the characters from the book (e.g. Elizabeth, Darcy, Bingley)\n",
    "- Visualize characters' occurences with regards to relative position in the book\n",
    "- Authomatically describe any character from the book\n",
    "- Find out which characters have been mentioned in a context of marriage\n",
    "- Build keywords extraction that could be used to display a word cloud (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-remark",
   "metadata": {},
   "source": [
    "To load the text file, it is convinient to decode using the utf-8 standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "catholic-envelope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 18;\n                var nbb_unformatted_code = \"def read_file(file_name):\\n    with open(file_name, \\\"r\\\", encoding=\\\"utf-8\\\") as file:\\n        return file.read()\";\n                var nbb_formatted_code = \"def read_file(file_name):\\n    with open(file_name, \\\"r\\\", encoding=\\\"utf-8\\\") as file:\\n        return file.read()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-sentence",
   "metadata": {},
   "source": [
    "### Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "peripheral-personality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 19;\n                var nbb_unformatted_code = \"text = read_file(\\\"data/pride_and_prejudice.txt\\\")\\n# Process the text\";\n                var nbb_formatted_code = \"text = read_file(\\\"data/pride_and_prejudice.txt\\\")\\n# Process the text\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = read_file(\"data/pride_and_prejudice.txt\")\n",
    "# Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed7b461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 20;\n                var nbb_unformatted_code = \"nlp = spacy.load('en_core_web_sm')\\nwith open ('data/pride_and_prejudice.txt') as f:\\n    text = f.read()\\ndoc = nlp(text)\";\n                var nbb_formatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nwith open(\\\"data/pride_and_prejudice.txt\\\") as f:\\n    text = f.read()\\ndoc = nlp(text)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open ('data/pride_and_prejudice.txt') as f:\n",
    "    text = f.read()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "painted-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5764 sentences in the book (Pride & Prejudice)\n",
      "[\n",
      "\n",
      "\"Why, my dear, you must know, Mrs. Long says that Netherfield is taken\n",
      "by a young man of large fortune from the north of England; that he came\n",
      "down on Monday in a chaise and four to see the place, and was so much\n",
      "delighted with it, that he agreed with Mr. Morris immediately; that he\n",
      "is to take possession before Michaelmas, and some of his servants are to\n",
      "be in the house by the end of next week., \"\n",
      "\n",
      "\"What is his name?\"\n",
      "\n",
      "\"Bingley., \"\n",
      "\n",
      "\"Is he married or single?\"\n",
      "\n",
      "\"Oh!, Single, my dear, to be sure!, A single man of large fortune; four or\n",
      "five thousand a year.]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"# How many sentences are in the book (Pride & Prejudice)?\\nlen_sent = len(list(doc.sents))\\nprint(f'There are {len_sent} sentences in the book (Pride & Prejudice)')\\n\\n# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\\nprint(list(doc.sents)[10:15])\";\n                var nbb_formatted_code = \"# How many sentences are in the book (Pride & Prejudice)?\\nlen_sent = len(list(doc.sents))\\nprint(f\\\"There are {len_sent} sentences in the book (Pride & Prejudice)\\\")\\n\\n# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\\nprint(list(doc.sents)[10:15])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many sentences are in the book (Pride & Prejudice)?\n",
    "len_sent = len(list(doc.sents))\n",
    "print(f'There are {len_sent} sentences in the book (Pride & Prejudice)')\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\n",
    "print(list(doc.sents)[10:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-devon",
   "metadata": {},
   "source": [
    "## Find all the personal names\n",
    "\n",
    "[Hint](# \"List doc.ents and check ent.label_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "secret-dryer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jane Austen, Jane Austen, Anonymous Volunteers, Jane Austen, Bennet, Bennet, Long, Bennet, Long, Netherfield, Morris, Bennet, Bingley, Bingley, William, Lady Lucas, Bingley, Lizzy, Lizzy, Jane, Lizzy, Bennet, Bennet, Bennet, Bingley, Bingley, Lizzy, Bingley, Elizabeth, Long, Long, Bennet, Bennet, Kitty, Lizzy, Long, Bingley, Bennet, Long, Bennet, Bingley, Bennet, Bennet, Bingley, Bennet, Bennet, Bingley, Lady Lucas, William, Bingley, Bennet, Bingley, Bennet, Bennet, Bingley, Bennet, Lady Lucas, Bingley, Bingley, Bingley, Hurst, Darcy, Bingley, Bingley, Darcy, Hurst, Bingley, Bennet, Elizabeth Bennet, Darcy, Darcy, Bingley, Elizabeth, Bingley, Darcy, Elizabeth, Bingley, Jane, Jane, Bingley, Catherine, Bennet, Bennet, Jane, Bingley, Lucas, Jane, Miss King, Maria Lucas, Jane, Lizzy, Bennet, Darcy, Lizzy, Jane, Elizabeth, Bingley, Elizabeth, Lizzy, Bingley, Bingley, Bingley, Bingley, Hurst, Bingley, Darcy, Bingley, Darcy, Darcy, Bingley, Darcy, Darcy, Bennet, Darcy, Bennet, Hurst, Bennet, William Lucas, Bennet, Elizabeth, Bennet, Lucas, Bingley, Jane, Robinson, Robinson, Robinson, Bennet, Darcy, Lizzy, Long, Jane, Darcy, Bingley, Jane, Long, Long, Long, Miss Lucas, Lizzy, Miss Lucas, Elizabeth, Darcy, Bennet, Hurst, Bingley, Jane, Elizabeth, Jane, Jane, Jane, Bingley, Jane, Jane, Jane, Elizabeth, Jane, Vingt, Charlotte, Jane, Bingley, Elizabeth, Darcy, William Lucas's, Darcy, Darcy, Lucas, Elizabeth, Darcy, Miss Lucas, Lucas, Darcy, Elizabeth, Darcy, William Lucas, William, Darcy, William, Darcy, Darcy, Lady Lucas, Elizabeth, Eliza, Darcy, Darcy, William, Darcy, William, Eliza, Darcy, Elizabeth, Eliza, Bingley, Bingley, Darcy, Elizabeth Bennet, Elizabeth Bennet, Bingley, Bennet, Phillips, Catherine, Phillips, Bingley, Carter, Bennet, Bennet, William, Forster, Carter, Watson, Bennet, Bennet, Bennet, Jane, Jane, Bingley, Jane, Louisa, Bennet, Jane, Elizabeth, Bingley, Bennet, Elizabeth, Jane, Jane, Jane, Bennet, Elizabeth, Bennet, Elizabeth, Bingley, Jane, Lizzy, Catherine, Carter, Elizabeth, Jane, Hurst, Elizabeth, Darcy, Elizabeth, Jane, Bingley, Elizabeth, Elizabeth, Jane, Elizabeth, Bingley, Jane, Bingley, Bingley, Jane, Jane, Jane, Bingley, Darcy, Jane, Hurst, Louisa, Elizabeth Bennet, Darcy, Bingley, Darcy, Bingley, Hurst, Jane Bennet, Elizabeth, Hurst, Eliza Bennet, Bingley, Elizabeth, Bingley, Darcy, Charles, Pemberley, Pemberley, Darcy, Charles, Caroline, Bingley, Darcy, Bingley, Elizabeth Bennet's, Charles, Darcy, Bingley, Elizabeth, Darcy, Hurst, Bingley, Hurst, Elizabeth, Bingley, Darcy, Bingley, Bingley, Jones, Jones, Bennet, Bingley, Bingley, Jane, Bennet, Jane, Bennet, Jane, Miss, Bingley, Bennet, Jones, Madam, Bingley, Bennet, Bennet, Bingley, Netherfield, Elizabeth, Lizzy, Darcy, Bennet, Darcy, Bennet, Bingley, Elizabeth, Darcy, Elizabeth, Darcy, Elizabeth, William, Bingley, Bingley, Lady Lucas, Jane, Jane, Elizabeth, Darcy, Elizabeth, Bingley, Jane, Lizzy, Bingley, Bennet, Bingley, Bingley, Jane, Carter, Bennet, Elizabeth, Jane, Darcy, Bingley, Hurst, Miss, Bingley, Darcy, Bingley, Hurst, Bingley, Hurst, Darcy, Darcy, Darcy, Darcy, Bingley, Charles, Bingley, Elizabeth, Darcy, Bingley, Elizabeth, Bingley, Darcy, Darcy, Miss Bennet, Darcy, Bingley, Miss Bennet, Darcy, Darcy, Elizabeth, Bingley, Elizabeth, Darcy, Bingley, Elizabeth, Bingley, Elizabeth, Hurst, Elizabeth, Darcy, Bingley, Darcy, Elizabeth, Miss Bennet, Bingley, Jane, Elizabeth, Darcy, Phillips, Elizabeth, Hurst, Elizabeth, Bingley, Hurst, Darcy, Darcy, Elizabeth, Jane, Elizabeth, Elizabeth, Jane, Darcy, Bennet, Hurst, Elizabeth, Hurst, Hurst, Darcy, Bingley, Hurst, Bennet, Charles, Darcy, Caroline, Bingley, Elizabeth, Eliza Bennet, Bingley, Darcy, Elizabeth, Darcy, Bingley, Elizabeth, Darcy, Darcy, Elizabeth, Bingley, Darcy, Bingley, Darcy, Darcy, Elizabeth, Bingley, Hurst, Darcy, Elizabeth, Elizabeth, Bennet, Jane, Elizabeth, Bingley, Elizabeth, Jane, Bingley, Jane, Bingley, Bennet, Jane, Darcy, Elizabeth, Netherfield, Miss, Bingley, Bingley, Jane, Elizabeth, Bennet, Jane, Jane, Catherine, Forster, Bennet, Bennet, Bingley, Collins, Elizabeth, Bennet, Bennet, Collins, Lady Catherine de\n",
      "Bourgh, Lewis de Bourgh, Lady Catherine, Bennet, Lady Catherine, Jane, Collins, Collins, Bennet, Collins, Bennet, Collins, Bennet, Bennet, Bennet, Lady\n",
      "Catherine de Bourgh's, Bennet, Collins, Lady Catherine, Lady Catherine, Bennet, Bennet, Lady Catherine, de Bourgh, Lady Catherine, Lady Catherine, Bennet, Bennet, Bennet, Collins, Phillips, Richard, Forster, Denny, Bennet, Bennet, Bennet, Collins, Bennet, Collins, Lady Catherine de\n",
      "Bourgh, Bennet, Bennet, Jane, Collins, Jane, Bennet, Elizabeth, Jane, Bennet, Mary, Collins, Bennet, Collins, Bennet, Collins, Collins, Denny, Denny, Wickham, Bingley, Darcy, Elizabeth, Elizabeth, Wickham, Darcy, Bingley, Denny, Wickham, Phillip, Lydia, Phillips, Phillips, Jones, Netherfield, Collins, Jane, Phillips, Denny, Wickham, Phillipses, Phillips, Elizabeth, Jane, Jane, Collins, Bennet, Phillips, Lady\n",
      "Catherine, Collins, Bennet, Wickham, Phillips, Lady Catherine's, Lady Catherine, Phillips, Wickham, Elizabeth, Wickham, Wickham, Elizabeth, Wickham, Collins, Phillips, Phillips, Wickham, Elizabeth, Wickham, Elizabeth, Darcy, Wickham, Netherfield, Darcy, Elizabeth, Wickham, Miss Bennet, Darcy, Elizabeth, Wickham, Netherfield, Wickham, Wickham, Darcy, Bennet, Darcy, Darcy, Wickham, Darcy, Elizabeth, Darcy, Darcy, Darcy, Wickham, Phillips, Darcy, Darcy, Darcy, Darcy, Elizabeth, Darcy, Wickham, Darcy, Elizabeth, Bingley, Bingley, Bingley, Darcy, Collins, Phillips, Phillips, Catherine de Bourgh, Wickham, Collins, Elizabeth, Lady Catherine de Bourgh, Collins, Lady Catherine de Bourgh, Lady Anne Darcy, Darcy, Lady Catherine's, de Bourgh, Elizabeth, Miss, Collins, Lady Catherine, Wickham, Phillips, Elizabeth, Wickham, Collins, Collins, Phillips, Jane, Wickham, Jane, Darcy, Wickham, Jane, Lizzy, Darcy, Bingley, Wickham, Darcy, Jane, Bingley, Bingley, Bennet, Elizabeth, Bennet, Bennet, Bingley, Jane, Darcy, Catherine, Elizabeth, Wickham, Collins, Bingley, Lady Catherine de Bourgh, Elizabeth, Jane, Wickham, Collins, Wickham, Collins, Elizabeth, Collins, Miss Bennets, Elizabeth, Wickham, Wickham, Darcy, Bingleys, Denny, Wickham, Darcy, Wickham, Darcy, Wickham, Bingley, Elizabeth, Collins, Wickham, Darcy, Darcy, Wickham, Elizabeth, Darcy, Darcy, Elizabeth, Elizabeth, Darcy, Wickham, Elizabeth, William Lucas, Eliza, Bingley, Darcy:--but, Darcy, William, Jane, William, William, Darcy, Miss Bennet, Darcy, Bingley, Eliza, George Wickham, Wickham, Darcy, Darcy, George Wickham, Darcy, Darcy, George Wickham, Darcy, Bingley, Elizabeth, Jane, Wickham, Jane, Wickham, Jane, Bingley, Darcy, Wickham, Darcy, Wickham, Darcy, Bingley, Wickham, Darcy, Darcy, Bingley, Elizabeth warmly, Jane, Bingley, Elizabeth, Lucas, Collins, de Bourgh, Lady Catherine de\n",
      "Bourgh, Darcy, Lady Catherine's, Darcy, Darcy, Collins, Elizabeth, Lady Catherine de Bourgh, Darcy, Collins, Collins, Darcy, Elizabeth, Darcy, Lady Catherine's, Elizabeth, Bingley, Jane, Lady Lucas, Jane, Bingley, Bennet, Jane, Jane, Lady Lucas, Elizabeth, Darcy, Darcy, Darcy, Darcy, Bennet, Lady Lucas, Elizabeth, Elizabeth, Elizabeth, Jane, Jane, Collins, Darcy, Bennet, Collins, Lady Lucas, Bingley, Darcy, Collins, Lucas, Collins, Darcy, Wickham, Bennet, Hurst, Bennet, Collins, Bingley, Darcy, Bennet, Bingley, Jane, Hurst, Bingley, Lydia, Bennet, Bingley, Bingley, Bennet, Collins, Bingley, Collins, Bennet, Elizabeth, Lizzy, Elizabeth, Collins, Lizzy, Elizabeth, Lizzy, Bennet, Collins, Elizabeth, Collins, Elizabeth, Jenkinson, de Bourgh's, Collins, Lady Catherine de Bourgh, Collins, Elizabeth, Lady Catherine, Lady Catherine, Collins, Collins, Collins, Collins, Elizabeth, Elizabeth, Collins, Bennet, Elizabeth, Collins, Bennet, Collins, Lizzy, Collins, Bennet, Lizzy, Bennet, Bennet, Lizzy, Collins, Bennet, Collins, Lizzy, Lizzy, Collins, Collins, Lizzy, Lizzy, Bennet rang, Elizabeth, Collins, Elizabeth, Bennet, Elizabeth, Collins, Bennet, Bennet, Bennet, Elizabeth, Jane, Jane, Elizabeth, Collins, Collins, Lizzy, Bennet, Miss Lucas, Lizzy, Miss Lucas, Bennet, Lizzy, Collins, Collins, Jane, Collins, Bennet, Collins, Bennet, Collins, Lucas, Bennet, Collins, Elizabeth, Wickham, Elizabeth, Darcy, Wickham, Bennet, Jane, Elizabeth, Jane, Jane, Caroline Bingley, Hurst, Elizabeth, Bingley, Jane, Bingley, Bingley, Jane, Bingley, Darcy, Charles, Jane, Lizzy, Jane, Bingley, Darcy, Jane, Bingley, Darcy, Miss de Bourgh, Jane, Bingley, Darcy, Bingley, Jane, Elizabeth, Jane, Elizabeth, Jane, Bingley, Bennet, Bingley, Lucas, Collins, Elizabeth, Elizabeth, Collins, Lucas, Lucas, Collins, Miss Lucas, William, Lady Lucas, Collins, Lady Lucas, Bennet, William, Collins, Charlotte, Collins, Elizabeth Bennet, Elizabeth, Collins, Bennet, Bennet, Lady Catherine's, Collins, Bennet, Lucas, Collins, Collins, Charlotte, Lucas, Collins, Elizabeth, Collins, Collins, Elizabeth, Collins, Collins, William Lucas, Bennet, William, Collins, Lizzy, William, William, Jane, Collins, Bennet, William, Collins, Elizabeth, William, Lady Lucas, Bennet, Charlotte Lucas, Jane, Elizabeth, Lucas, Collins, Bennet, Bennet, Elizabeth, Collins, Lucas, Lady Catherine, Collins, Bennet, Bennet, Bingley, Jane, Elizabeth, Elizabeth, Jane, Jane, Elizabeth, Elizabeth, Jane, Jane, Collins, Bennet, Lucas, Collins, Bennet, Bennet, Bennet, Bennet, Collins, Bennet, Jane, Darcy, Darcy, Jane, Darcy, Jane, Jane, Jane, Elizabeth, Bennet, Netherfield, Jane, Jane, Elizabeth, Bennet, Elizabeth, Lizzy, Collins, Jane, Collins, Charlotte Lucas, Jane, Lizzy, Bingley, Elizabeth, Darcy, Jane, Bingley, Bennet, Elizabeth, Jane, Bennet, Bingley, Bennet, Lizzy, Jane, Wickham, Jane, Bennet, Wickham, Elizabeth, Darcy, Darcy, Bennet, Darcy, Collins, Bennet, Gardiner, Gardiner, Bennet, Phillips, Gardiner, Bennet, Jane, Jane, Bingley, Lizzy, Collins, Lady Lucas, Gardiner, Jane, Elizabeth, Elizabeth, Jane, Bingley, Elizabeth, Bingley, Lizzy, Gardiner, Darcy, Jane, Darcy, Bingley, Jane, Elizabeth, Jane, Jane, Bennet, Bennet, Wickham, Gardiner, Elizabeth, Elizabeth, Gardiner, Wickham, Wickham, Darcy, Gardiner, Darcy, Wickham, Darcy, Fitzwilliam, Gardiner, Elizabeth, Lizzy, Wickham, Wickham, Darcy, Wickham, Elizabeth, Elizabeth, Collins, Gardiners, Jane, Bennet, Elizabeth, Kent, Maria, Elizabeth, Elizabeth, Lady Catherine, Elizabeth, Lady Catherine's, Collins, Elizabeth, Elizabeth, Jane, Bingley, Caroline, Darcy, Darcy, Hurst, Bingley, Jane, Bingley, Jane, Lizzy, Bingley, Darcy, Bingley, William, Maria, Elizabeth, Jane, Jane, Gardiner, Elizabeth, Elizabeth, Elizabeth, Elizabeth, Gardiner, Miss King, Collins, Jane, William, Wickham, Lady Catherine de Bourgh, William Lucas, Maria, William, Gardiner, Jane, Elizabeth, Jane, Gardiner, Bingley, Jane, Gardiner, Wickham, Elizabeth, Miss King, Miss King, Elizabeth, Lizzy, Lizzy, Gardiner, Elizabeth, Elizabeth, Elizabeth, Collins, Collins, Elizabeth, Collins, Collins, Elizabeth, Collins, William, Collins, Elizabeth, Lady Catherine, Collins, Elizabeth, Lady Catherine\n",
      "de Bourgh, Maria, Lady Catherine, Elizabeth, Collins, Maria, Maria, Elizabeth, Lady Catherine, Maria, Lady Catherine, Jenkinson, Miss de Bourgh, Charlotte, Miss de Bourgh, Elizabeth, Collins, William, Elizabeth, de Bourgh, Collins, Collins, William, Collins, Lady\n",
      "Catherine, Lady Catherine, Lady Catherine, Maria Lucas, Collins, Lewis de Bourgh, Maria, William, Elizabeth, Collins, Jenkinson, Collins, William, Elizabeth, Lady Catherine, Wickham, Elizabeth, Lady Catherine, Darcy, Maria, Miss de Bourgh, Jenkinson, Collins, Lady Catherine, Collins, Elizabeth, Lady Catherine, Lady Catherine, Elizabeth, Miss de Bourgh, Lady Catherine, Jenkinson, de Bourgh, Maria, Lady Catherine, Elizabeth, Collins, Maria, Collins, Elizabeth, Lady Catherine, Collins, Lewis de Bourgh's, Miss, Jenkinson, Collins, Lady Metcalf's, Pope, Lady\n",
      "Catherine, Bennet, Elizabeth, Lady Catherine, Elizabeth, Lady Catherine, William, Collins, de Bourgh, Jenkinson, Jenkinson, de Bourgh's, Lady Catherine, Collins, William, Lady Catherine, Collins, Lady Catherine, William, Elizabeth, Collins, William, William, Collins, Elizabeth, Collins, Collins, de Bourgh, Collins, Elizabeth, Collins, Collins, William, Collins, Elizabeth, Lady Catherine, Lady Catherine's, Elizabeth, Darcy, Bingley, Lady Catherine, Collins, Lady Catherine, Darcy, Fitzwilliam, Collins, Darcy, Fitzwilliam, Darcy, Collins, Elizabeth, Fitzwilliam, Collins, Jane, Fitzwilliam, Lady Catherine, Fitzwilliam, Darcy, Lady Catherine's, Fitzwilliam, Collins, Elizabeth, Lady Catherine, Darcy, Fitzwilliam, Darcy, Darcy, Lady\n",
      "Catherine, Bennet, Collins, Jenkinson, Darcy, Fitzwilliam, Elizabeth, Lady Catherine, Darcy, Fitzwilliam, Darcy, Fitzwilliam, Darcy, Fitzwilliam, Darcy, Elizabeth, Fitzwilliam, Fitzwilliam, Darcy, Elizabeth, Lady Catherine, Lady Catherine, Bennet, Anne, Anne, Darcy, Bingley, Lady Catherine, Elizabeth, Jane, Collins, Maria, Lady Catherine, Darcy, Darcy, Darcy, Bingley, Bingley, Netherfield, Bingley, Darcy, Lady Catherine, Collins, Collins, Elizabeth, Collins, Elizabeth, Jane, Collins, Darcy, Darcy, Miss Bennet, Elizabeth, Lady Catherine, Fitzwilliam, Elizabeth, George Wickham, Fitzwilliam, Darcy, Collins, Fitzwilliam, Elizabeth, Elizabeth, Elizabeth, Fitzwilliam, Darcy, Elizabeth, Darcy, Collins, Fitzwilliam, Jane, Jane, Darcy, Fitzwilliam, Darcy, Darcy, Fitzwilliam, Elizabeth, Colonel Fitzwilliam, Darcy, Darcy, Darcy, Hurst, Bingley, Elizabeth drily, Darcy, Darcy, Bingley, Bingley, Darcy, Bingley, Darcy, Fitzwilliam, Fitzwilliam, Darcy, Fitzwilliam, Darcy, Darcy, Bingley, Jane, Bingley, Jane, Fitzwilliam, Jane, Darcy, Darcy, Bingley, Darcy, Collins, Collins, Lady Catherine's, Elizabeth, Darcy, Jane, Darcy, Jane, Fitzwilliam, Fitzwilliam, Darcy, Elizabeth, Darcy, Elizabeth, Darcy, Wickham, Darcy, Darcy, Elizabeth, Darcy, Elizabeth, Darcy, Jane, Lady Catherine's, Darcy, Darcy, Bingley, Wickham, William Lucas's, Bingley, Wickham, Wickham, George Wickham, Wickham, Wickham, Wickham, Wickham, Colonel Fitzwilliam, Wickham, Younge, Wickham, Younge, Wickham, Colonel Fitzwilliam, Elizabeth, Darcy, Wickham, Pemberley, Darcy, Wickham, Darcy, Wickham, Darcy, Darcy, Fitzwilliam, Fitzwilliam, Darcy, Wickham, Phillips, Darcy, Darcy, Darcy, Miss King, Darcy, Bingley, Jane, Wickham, Wickham, Bingley, Darcy, Wickham, Jane, Darcy, Jane, Jane, Jane, Darcy, Fitzwilliam, Elizabeth, Fitzwilliam, Collins, Lady Catherine, Lady Catherine, Rosings, Lady Catherine, Collins, Lady Catherine, Bennet, Collins, Elizabeth, Collins, Bennet, Lady Catherine, Collins, Darcy, Darcy, Lady Anne, Collins, Bromley, Lady Catherine, Elizabeth, Darcy, Jane, Catherine, Catherine, Jane, Darcy, Bingley, Jane, Jane, Maria, Lady Catherine, Miss de Bourgh, Collins, Elizabeth, Collins, Collins, Lady Catherine's, Elizabeth, Lady Catherine's, Collins, Elizabeth, Charlotte, Elizabeth, Collins, Gardiner, Maria, Maria, Gardiner, Elizabeth, Jane, Darcy, Jane, Bennet, Kitty, Elizabeth, Elizabeth, Brighton, Elizabeth, Wickham, Mary King, Wickham, Mary King, Elizabeth, Jane, snug, Jane, Phillips, Lizzy, Forster, Forster, Harriet, Elizabeth, Bennet, Jane, Bennet, Elizabeth, Lizzy, Maria, Lady Lucas, Maria, Bennet, Jane, George, Elizabeth, Wickham, Brighton, Elizabeth, Jane, Darcy, Bennet, Darcy, Elizabeth, George Wickham, Jane, Darcy, Elizabeth, Jane, Wickham, Darcy, Lizzy, Wickham, Darcy, Lizzy, Jane, Wickham, Darcy, Wickham, Bennet, Darcy, Darcy, Wickham, Elizabeth, Darcy, Jane, Lizzy, Bennet, Jane, Phillips, Jane, Netherfield, Jane, Elizabeth, Lizzy, Miss Bennets, Lizzy, Bennet, Phillips, Elizabeth, Darcy, Forster, Forster, Bennet, Forster, Elizabeth, Jane, Elizabeth, Forster, Elizabeth, Bennet, Lizzy, Bennet, Jane, Lydia, Brighton, Forster, Brighton, Elizabeth, Brighton, Brighton, Wickham, Elizabeth, Fitzwilliam, Darcy, Darcy, Wickham, Elizabeth, Wickham, Wickham, Darcy, de Bourgh, Elizabeth, Forster, Elizabeth, Bennet, Elizabeth, Wickham, Jane, Forster, Bennet, Elizabeth, Gardiner, Gardiner, Gardiner, Pemberley, Gardiner, Jane, Elizabeth, Gardiner, Elizabeth, Pemberley, Gardiner, Gardiner, Elizabeth, Wickham, Gardiner, Darcy, Pemberley, Elizabeth, Elizabeth, Pemberley, Elizabeth, Elizabeth, Elizabeth, Wickham, Gardiner, Elizabeth, Reynolds, Gardiner, Lizzy, Reynolds, Elizabeth, Darcy, Elizabeth, Wickham, Reynolds, Darcy, Darcy, Gardiner, Gardiner, Reynolds, Darcy, Elizabeth, Gardiner, Darcy, Gardiner, Gardiner, Elizabeth, Darcy, Elizabeth, Reynolds, Darcy, Elizabeth, Elizabeth, Darcy, Reynolds, Elizabeth, Reynolds, Elizabeth, Darcy, Elizabeth, Darcy, Gardiner, Gardiner, Gardiner, Elizabeth, Darcy, Elizabeth, Gardiner, Gardiner, Elizabeth, Darcy, Gardiner, Elizabeth, Gardiner, Elizabeth, Darcy, Bingley, Bingley, Darcy, Gardiner, Gardiner, Darcy, Elizabeth, Lizzy, Wickham, Gardiner, Wickham, Wickham, Wickham, Gardiner, Elizabeth, Darcy, Darcy, Pemberley, Elizabeth, Darcy, Elizabeth, Darcy, Darcy, Elizabeth, Elizabeth, Darcy, Darcy, Elizabeth, Gardiner, Darcy, Darcy, Darcy, Jane, Jane, Darcy, Darcy, Gardiner, Miss Bennet, Darcy, Gardiner, Elizabeth, Elizabeth, Elizabeth, Gardiner, Darcy, Darcy, Darcy, Darcy, Elizabeth, Darcy, Gardiner, Elizabeth, Bingley, Darcy, Hurst, Bingley, Gardiner, Hurst, Bingley, Annesley, Gardiner, Elizabeth, Darcy, Bingley, Darcy, Miss, Elizabeth, Annesley, Darcy, Elizabeth, Darcy, Gardiner, Elizabeth, Bingley, Darcy, Darcy, Elizabeth, Miss, Eliza, Wickham, Elizabeth, Darcy, Bingley, Darcy, Darcy, Elizabeth, Elizabeth, Elizabeth, Bingley, Darcy, Miss, Bingley, Elizabeth, Elizabeth, Darcy, Bingley, Eliza Bennet, Darcy, Louisa, Darcy, Bingley, Darcy, Elizabeth, Darcy, Bingley, Gardiner, Elizabeth, Elizabeth, Gardiner, Gardiner, Jane, Elizabeth, Jane, Lizzy, Wickham, Lizzy, Forster, Elizabeth, Lizzy, Wickham, Forster, F., Lydia, F., Hatfield, F., Lizzy, W., Kitty, Lizzy, Forster, Brighton, Elizabeth, Darcy, Gardiner, Gardiner, Darcy, Darcy, Jane, Wickham, Brighton, Jane, Elizabeth, Elizabeth, Darcy, Elizabeth, Elizabeth, Jane, Wickham, Jane, Wickham, Lydia, Jane, Gardiner, Lydia, Gardiner, Gardiner, Elizabeth, Pemberley, Gardiner, Darcy, Elizabeth, Elizabeth, Elizabeth, Elizabeth, Gardiner, Wickham, Lizzy, Gardiner, Jane, Wickham, Elizabeth, Wickham, Jane, Wickham, Jane, Jane, Wickham, Gardiner, Elizabeth, Darcy, Pemberley, Darcy, Jane, Darcy, Fitzwilliam, Jane, Forster, Elizabeth, Elizabeth, Jane, Jane, Jane, Elizabeth, Gardiner, Jane, Elizabeth, Jane, Bennet, Bennet, Gardiner, Bennet, Bennet, Lydia, Gardiner, Elizabeth, Miss Bennets, Elizabeth, Jane, Elizabeth, Miss, Forster, Denny, Wickham, Brighton, Wickham, Wickham, Jane, Forster, Elizabeth, Gretna Green, Lydia Wickham, Sally, Elizabeth, Jane, Elizabeth, Phillips, Lady Lucas, Elizabeth, Jane, Bennet, Bennet, Gardiner, Bennet, Elizabeth, Jane, Gardiner, Bennet, Bennet, Gardiner, Wickham, Forster, Lizzy, Gardiner, Collins, Jane, Elizabeth, Collins, Charlotte, Bennet, Collins, Lady Catherine, Lady Catherine, Gardiner, Wickham, Forster, Gardiner, Longbourn, Jane, Gardiner, Bennet, Gardiner, Bennet, Gardiner, Elizabeth, Gardiner, Elizabeth, Bennet, Elizabeth, Elizabeth, Lizzy, Lizzy, Miss Bennet, Kitty, Brighton, Brighton, Bennet, Jane, Elizabeth, Hill, Hill, Gardiner, Jane, Jane, Wickham, Elizabeth, Wickham, Elizabeth, Jane, Elizabeth, Jane, Lydia, Elizabeth, Wickham, Bennet, Elizabeth, Jane, Lydia, Wickham, Gardiner, Wickham, Jane, Elizabeth, Bennet, Bennet, Jane, Gardiner, Wickham, Gardiner, Lizzy, Wickham, Wickham, Jane, Jane, Lady Lucas, Long, Hill, Lydia, Hill, Bennet, Bennet, Bennet, Bennet, Bennet, Wickham, Lydia Bennet, Bennet, Jane, Pulvis Lodge, Bennet, Bennet, Bennet, Wickham, Darcy, Darcy, Wickham, Gardiner, Bennet, Wickham, Wickham, Wickham, Bennet, Wickham, Gardiner, Bennet, Forster, Jane, Elizabeth, Bennet, Elizabeth, Wickham, Jane, Elizabeth, Miss Bennets, Jane, Bennet, Bennet, Elizabeth, Wickham, Jane, Wickham, Jane, Elizabeth, William Goulding, Jane, Phillips, Wickham, Hill, Lydia, Elizabeth, Wickham, Bennet, Wickham, Elizabeth, Wickham, Elizabeth, Lizzy, Elizabeth, Wickham, Wickham, Stone, Darcy, Darcy, Elizabeth, Wickham, Wickham, Jane, Elizabeth, Wickham, Elizabeth, Darcy, Elizabeth, Lydia, Elizabeth, Darcy, Wickham, Wickham, Wickham, Younge, Darcy, Younge, Wickham, Wickham, Wickham, Wickham, Wickham, Darcy, Bennet, Wickham, Wickham, Darcy, Gracechurch, Gardiner, Darcy, Lizzy, Lizzy, Jane, Wickham, Lizzy, Wickham, Jane, Elizabeth, Jane, Darcy, Lizzy, Elizabeth, Darcy, Wickham, Darcy, Bennet, Elizabeth, Darcy, de Bourgh, Elizabeth, Darcy, Wickham, Wickham, Elizabeth, Bennet, Lydia, Wickham, Bennet, William Lucas, Bennet, Elizabeth, Bennet, Jane, Bingley, Nicholls, Bennet, Elizabeth, Lizzy, Jane, Elizabeth, Bingley, Bennet, Long, Bingley, Jane, Elizabeth, Bingley, Bennet, Jane, Elizabeth, Darcy, what's, Darcy!--and, Bingley, Darcy, Bingley, Elizabeth, Jane, Gardiner, Jane, Jane, Elizabeth, Gardiner, Jane, Bingley, Bennet, Lucas, George Wickham, Lydia Bennet, Gardiner, Elizabeth, Darcy, Bingley, Darcy, Bingley, Bingley, Jane, Jane, Bennet, Bingley, Bennet, Elizabeth, Darcy, Elizabeth, Elizabeth, Jane, Lizzy, Bennet, Elizabeth, Bingley, Jane, Darcy, Jane, Darcy, Elizabeth, Bennet, Elizabeth, Pemberley, Annesley, Elizabeth, Elizabeth, Bennet, Darcy, Jane, Long, Bennet, Long, Bennet, Jane, Elizabeth, Lizzy, Bingley, Bennet, Bennet, Jane, Bingley, Sarah, Miss, Lizzy, Jane, Jane, Bennet, Bennet, Elizabeth, Catherine, Elizabeth, Jane, Lizzy, Bennet, Bingley, Jane, Elizabeth, Darcy, Bennet, Bingley, Bennet, Elizabeth, Bingley, Elizabeth, Jane, Lizzy, Jane, Jane, Bennet, Bennet, Bingley, Bennet, Jane, Bennet, Jane, Jane, Jane, Jane, Elizabeth, Bingley, Jane, Elizabeth, Elizabeth, Bingley, Lizzy, Jane, Jane, Jane, Lizzy, Collins, Bennet, Phillips, Jane, Lady Catherine de Bourgh, Elizabeth, Elizabeth, Elizabeth, Bennet, Elizabeth, Bennet, Bennet, Lady Catherine, Lady Catherine, William Lucas's, Bennet, Collins, Bennet, Lady Catherine, Elizabeth, Lady Catherine, Elizabeth, Elizabeth, Lady Catherine, Miss Bennet, Elizabeth Bennet, Darcy, Elizabeth, Elizabeth, Miss Bennet, Darcy, Lady Catherine, de Bourgh, Miss de Bourgh, Darcy, Elizabeth, Miss Bennet, Elizabeth, Lady\n",
      "Catherine, Lady Catherine, Darcy, Lady Catherine, Lady Catherine, Lady Catherine, Elizabeth, Darcy, Miss Bennet, Lady Catherine, Lizzy, Lady Catherine, Elizabeth, Jane, Lady Catherine, Lady Catherine's, Elizabeth, Netherfield, Bennet, Lizzy, Lady Catherine, Elizabeth, Collins, Collins, Collins, Lizzy, Lizzy, Lady Catherine de Bourgh, Darcy, Lizzy, Darcy, Lizzy, Elizabeth, Collins, Wickham, Lizzy, Lady Catherine, Elizabeth, Darcy, Bingley, Lady Catherine, Bennet, Jane, Bennet, Jane, Elizabeth, Maria, Elizabeth, Darcy, Darcy, Gardiner, Elizabeth, Elizabeth, Lady Catherine, Elizabeth, Darcy, Elizabeth, Darcy, Bingley, Jane, Darcy, Elizabeth, Bingley, Bingley, Lizzy, Elizabeth, Jane, Darcy, Elizabeth, Jane, Jane, Bennet, Lizzy, Darcy, Lizzy, Bennet, Elizabeth, Jane, Lizzy, Jane, Lizzy, Bingley, Jane, Bennet, Bingley, Lizzy, Bennet, Darcy, Bingley, Lizzy, Bingley, Bennet, Lizzy, Darcy, Lizzy, Bennet, Darcy, Bingley, Kitty, Darcy, Elizabeth, Bennet, Lizzy, Jane, Bennet, Darcy, Bennet, Darcy, Darcy, Lizzy, Darcy, Jane, Elizabeth, Lizzy, Lizzy, Darcy, Lizzy, Darcy, Darcy, Collins, Bennet, Darcy, Lizzy, Jane, Lizzy, Lizzy, Darcy, Elizabeth, Bennet, Bennet, Wickham, Jane, Elizabeth, Jane, Catherine, Lady Catherine, Netherfield, Lady Catherine, Elizabeth, Darcy, Elizabeth, Gardiner, Jane, Darcy, Pemberley, Darcy, Lady Catherine, Bennet, Collins, Elizabeth, Darcy, Lady Catherine, Jane, Jane, Darcy, Collins, Elizabeth, Lady Catherine, Elizabeth, Darcy, William Lucas, William, Phillips, Phillips, Elizabeth, Bennet, Bingley, Darcy, Bennet, Pemberley, Bingley, Jane, Jane, Elizabeth, Wickham, Bennet, Wickham, Darcy, Elizabeth, Darcy, Wickham, Wickham, Darcy, Elizabeth, Jane, Darcy, Elizabeth, Bath, Bingley, Bingley, Darcy, Darcy, Elizabeth, Darcy, Elizabeth, Lady Catherine, Elizabeth, Elizabeth, Elizabeth, Jane Austen\n",
      "\n",
      ", Anonymous Volunteers, Project Gutenberg-tm, Gutenberg, Project Gutenberg-tm, Project Gutenberg, Project Gutenberg-tm, Project Gutenberg-tm, Project Gutenberg-tm, Project Gutenberg-tm, Project Gutenberg-tm, Project Gutenberg-tm, Project Gutenberg, Project Gutenberg-tm, Project Gutenberg-tm, Gutenberg, Project Gutenberg-tm\n",
      ", Project Gutenberg-tm\n",
      ", Project Gutenberg-tm's, Gregory B. Newby, Michael S. Hart, Project Gutenberg-tm]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 38;\n                var nbb_unformatted_code = \"# Extract all the personal names from Pride & Prejudice and count their occurrences.\\n# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\\nimport re\\nfrom collections import Counter, defaultdict\\n\\n\\ndef find_character_occurences(doc):\\n    \\\"\\\"\\\"\\n    Return a list of actors from `doc` with corresponding occurences.\\n\\n    :param doc: Spacy NLP parsed document\\n    :return: list of tuples in form\\n        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\\n    \\\"\\\"\\\"\\n\\n    characters = Counter()\\n    # your code here\\n    sents = nlp(doc)\\n    name_list = []\\n    for names in sents.ents:\\n        if names.label_ == 'PERSON':\\n            name_list.append(names)\\n    #name_list = [names for names in sents.ents if names.label_ == 'PERSON']\\n    print(name_list)\\n    \\n\\n\\nfind_character_occurences(text)\\n# print(find_character_occurences(processed_text)[:20])\\n\\n\\n# import spacy                                                                                                                            \\n\\n# nlp = spacy.load('en')                                                                                                                  \\n# sents = nlp(document_string) \\n#  [ee for ee in sents.ents if ee.label_ == 'PERSON']      \";\n                var nbb_formatted_code = \"# Extract all the personal names from Pride & Prejudice and count their occurrences.\\n# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\\nimport re\\nfrom collections import Counter, defaultdict\\n\\n\\ndef find_character_occurences(doc):\\n    \\\"\\\"\\\"\\n    Return a list of actors from `doc` with corresponding occurences.\\n\\n    :param doc: Spacy NLP parsed document\\n    :return: list of tuples in form\\n        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\\n    \\\"\\\"\\\"\\n\\n    characters = Counter()\\n    # your code here\\n    sents = nlp(doc)\\n    name_list = []\\n    for names in sents.ents:\\n        if names.label_ == \\\"PERSON\\\":\\n            name_list.append(names)\\n    # name_list = [names for names in sents.ents if names.label_ == 'PERSON']\\n    print(name_list)\\n\\n\\nfind_character_occurences(text)\\n# print(find_character_occurences(processed_text)[:20])\\n\\n\\n# import spacy\\n\\n# nlp = spacy.load('en')\\n# sents = nlp(document_string)\\n#  [ee for ee in sents.ents if ee.label_ == 'PERSON']\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count their occurrences.\n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def find_character_occurences(doc):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "\n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "\n",
    "    characters = Counter()\n",
    "    # your code here\n",
    "    sents = nlp(doc)\n",
    "    name_list = []\n",
    "    for names in sents.ents:\n",
    "        if names.label_ == 'PERSON':\n",
    "            name_list.append(names)\n",
    "   \n",
    "    print(name_list)\n",
    "    \n",
    "\n",
    "\n",
    "find_character_occurences(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-turtle",
   "metadata": {},
   "source": [
    "## Plot characters personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-entity",
   "metadata": {},
   "source": [
    "We can investigate where a particular entity occurs in the text. We can do it just accessing the `.start` attribute of an entity:\n",
    "\n",
    "[Hint](# \"ent.start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the start positions of person entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-stanford",
   "metadata": {},
   "source": [
    "So we can create a function that stores all the offsets of every character:\n",
    "   \n",
    "   \n",
    "[Hint](# \"Create a dictionary with the lowered lemmas [ent.lemma_.lower()] and associate a list of all the ent.starts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot characters' mentions as a time series relative to the position of the actor's occurrence in a book.\n",
    "\n",
    "def get_character_offsets(doc):\n",
    "    \"\"\"\n",
    "    For every character in a `doc` collect all the occurences offsets and store them into a list. \n",
    "    The function returns a dictionary that has actor lemma as a key and list of occurences as a value for every character.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: dict object in form\n",
    "        {'elizabeth': [123, 543, 4534], 'darcy': [205, 2111]}\n",
    "    \"\"\"\n",
    "            \n",
    "    return dict(character_offsets)\n",
    "\n",
    "character_occurences = get_character_offsets(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-companion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:01:35.087781Z",
     "start_time": "2021-03-28T17:01:35.071546Z"
    }
   },
   "source": [
    "[Hint](# \"Use the character offsets for each character as x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the character occurrences in the whole text\n",
    "NUM_BINS = 20\n",
    "\n",
    "def plot_character_hist(character_offsets, character_label, cumulative=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"elizabeth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"darcy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-sunset",
   "metadata": {},
   "source": [
    "### Cumulative occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"elizabeth\", cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"darcy\", cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-senior",
   "metadata": {},
   "source": [
    "### Spacy parse tree in action\n",
    "\n",
    "[Hint](# \"ent.subtree, token.pos_ == 'ADJ'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr. Darcy.\n",
    "\n",
    "def get_character_adjectives(doc, character_lemma):\n",
    "    \"\"\"\n",
    "    Find all the adjectives related to `character_lemma` in `doc`\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :param character_lemma: string object\n",
    "    :return: list of adjectives related to `character_lemma`\n",
    "    \"\"\"\n",
    "    \n",
    "    adjectives = []\n",
    "    for ent in processed_text.ents:\n",
    "        # your code here\n",
    "        pass\n",
    "    \n",
    "     for ent in processed_text.ents:\n",
    "        if ent.lemma_.lower() == character_lemma:\n",
    "            if ent.root.dep_ == 'nsubj':\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == 'acomp':\n",
    "                        adjectives.append(child.lemma_)\n",
    "                        \n",
    "    return adjectives\n",
    "\n",
    "print(get_character_adjectives(processed_text, 'darcy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Elizabeth.\n",
    "\n",
    "\n",
    "print(get_character_adjectives(processed_text, 'elizabeth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-collective",
   "metadata": {},
   "source": [
    "For all the dependencies manual: https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
    "\n",
    "`acomp`: adjectival complement\n",
    "*i.e.* an adjectival phrase which functions as the complement (like an object of the verb) e.g. \"She looks very beautiful\": *beautiful* is an adjectival complement of *looks*\n",
    "\n",
    "`nsubj`: nominal subject\n",
    "*i.e.* a noun phrase which is the syntactic subject of a clause. The head of this relation\n",
    "might not always be a verb: when the verb is a copular verb, the root of the clause is the complement of\n",
    "the copular verb, which can be an adjective or noun.\n",
    "*e.g.* \"Clinton defeated Dole\". The relationship is *nsubj(defeated, Clinton)*\n",
    "\n",
    "\"The baby is cute\". The relationship is *nsubj(cute, baby)*.\n",
    "\n",
    "In the code, `.dep_`stands for syntactic dependency, *i.e.* the relation between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text.ents[30].root.dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-crown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:09:34.144121Z",
     "start_time": "2021-03-28T17:09:34.132840Z"
    }
   },
   "source": [
    "[Hint](# \"ent.label_, ent.root.head.lemma_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "\n",
    "character_verb_counter = Counter()\n",
    "\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if # your code here:\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10)) \n",
    "\n",
    "# do the same for talking and doing\n",
    "\n",
    "print(character_verb_counter.most_common(10)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-hawaiian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:10:42.811139Z",
     "start_time": "2021-03-28T17:10:42.804815Z"
    }
   },
   "source": [
    "[Hint](# \"ent.label_, ent.root.head.pos_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 20 most used verbs\n",
    "verb_counter = Counter()\n",
    "\n",
    "# your code here\n",
    "\n",
    "print(verb_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the most used verb and how many time a character used the verb\n",
    "\n",
    "import pandas as pd\n",
    "verb_characters = {}\n",
    "verb_list = [verb[0] for verb in verb_counter.most_common(20)]\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ in verb_list:\n",
    "        # complete the code\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(verb_characters).transpose().fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the less meaningful columns\n",
    "df = df[df.columns[df.sum()>=10]].sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.heatmap(df, annot=True, cmap='Blues')\n",
    "df.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "bd0e965e62c25b8055962743e9c50cc1ad51895dd9a75e3af7b7f576f1af3764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
